# Cloud Deployment Guide

This guide explains how to properly configure the Enhanced Research Agent for both local and cloud environments.

## üè† **Local Environment (Current Setup)**

### Configuration
- **Model**: `gemma2:2b` (via Ollama)
- **Config**: `.env` file
- **Dependencies**: All installed locally

### How it works
```python
# The app automatically detects it's running locally
environment = "local"
model = "gemma2:2b"  # Uses your local Ollama
```

## ‚òÅÔ∏è **Streamlit Cloud Environment**

### Configuration
- **Model**: `microsoft/Phi-3-mini-4k-instruct` (via Hugging Face)
- **Config**: Streamlit secrets
- **Dependencies**: Installed from `requirements.txt`

### How it works
```python
# The app automatically detects it's running in cloud
environment = "cloud"
model = "microsoft/Phi-3-mini-4k-instruct"  # Uses Hugging Face
```

## üîß **Configuration Priority**

The app uses this priority order for configuration:

1. **Environment Variables** (highest priority)
2. **Streamlit Secrets** (cloud only)
3. **Default Values** (fallback)

### Environment Variables
```bash
# Set these in your environment
export LLM_MODEL="microsoft/Phi-3-mini-4k-instruct"
export OPENAI_API_KEY="your-key-here"
export LANGFUSE_PUBLIC_KEY="your-key-here"
export LANGFUSE_SECRET_KEY="your-key-here"
```

### Streamlit Secrets
In Streamlit Cloud, add these to your secrets:

```toml
# .streamlit/secrets.toml
LLM_MODEL = "microsoft/Phi-3-mini-4k-instruct"
OPENAI_API_KEY = "your-openai-api-key"
LANGFUSE_PUBLIC_KEY = "your-langfuse-public-key"
LANGFUSE_SECRET_KEY = "your-langfuse-secret-key"
LANGFUSE_HOST = "https://us.cloud.langfuse.com"
```

## üöÄ **Deployment Steps**

### 1. **Prepare for Cloud**
```bash
# The app will automatically use the right model
# No code changes needed!
```

### 2. **Deploy to Streamlit Cloud**
1. Go to [Streamlit Cloud](https://share.streamlit.io/)
2. Connect your GitHub repository
3. Set main file: `enhanced_agent_streamlit.py`
4. Add secrets (optional, for full functionality)

### 3. **Test Both Environments**
```bash
# Local (uses gemma2:2b)
streamlit run enhanced_agent_streamlit.py

# Cloud (uses microsoft/Phi-3-mini-4k-instruct)
# Deploy to Streamlit Cloud
```

## üìä **Model Comparison**

| Environment | Model | Size | Speed | Reasoning | Best For |
|-------------|-------|------|-------|-----------|----------|
| Local | gemma2:2b | 2B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Private, fast |
| Cloud | Phi-3-mini | 3.8B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Public, powerful |

## üîç **Troubleshooting**

### Common Issues

1. **"No module named 'dotenv'"**
   - ‚úÖ **Fixed**: App now handles missing dotenv gracefully
   - **Solution**: Uses environment variables instead

2. **"Model not found"**
   - ‚úÖ **Fixed**: App automatically selects appropriate model
   - **Solution**: Uses Hugging Face models in cloud

3. **"Configuration not loaded"**
   - ‚úÖ **Fixed**: App uses multiple configuration sources
   - **Solution**: Falls back to defaults if needed

### Debug Information
The app will show:
```
üåç Environment: local/cloud
ü§ñ Using model: gemma2:2b/microsoft/Phi-3-mini-4k-instruct
‚úÖ Configuration helper loaded
```

## üéØ **Expected Behavior**

### Local Environment
- ‚úÖ Uses `gemma2:2b` via Ollama
- ‚úÖ Loads `.env` file
- ‚úÖ Full DSPy structured reasoning
- ‚úÖ Langfuse tracing
- ‚úÖ MCP information gathering

### Cloud Environment
- ‚úÖ Uses `microsoft/Phi-3-mini-4k-instruct` via Hugging Face
- ‚úÖ Loads Streamlit secrets
- ‚úÖ Full DSPy structured reasoning
- ‚úÖ Langfuse tracing (if API keys provided)
- ‚úÖ MCP information gathering

## üí° **Key Benefits**

1. **Automatic Detection**: App knows which environment it's in
2. **Graceful Fallbacks**: Works even with missing dependencies
3. **No Code Changes**: Same code works in both environments
4. **Optimal Performance**: Uses best model for each environment
5. **Easy Deployment**: Just push to GitHub and deploy

The app is now **truly cloud-ready**! üéâ
